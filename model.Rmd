---
title: "Store Random Effect Model"
author: "Jinghan Cui"
date: "12/1/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(lme4)
library(rstanarm)
library(arm)
library(emmeans)
library(VGAM)
```


## 2. Store Random Effect Models
### 2.1 Variable Selection
```{r}
sample_store_bars <- read.csv("sample_store_bars.csv")
# check correlations within predictor variables
ggpairs(sample_store_bars[, c("review_count","useful_pct","fans","average_stars", "store_review_count")])
# we may want to drop fans as it is highly correlated with review_count
```

### 2.2 EDA
```{r}
# the sampled restaurants have same store_stars while the plot shows a different distribution of reveiw stars per restaurant
ggplot(sample_store_bars, aes(x = store_name, y = stars)) +
  stat_sum(aes(size = ..n.., group = 1), color = "tomato3") 

# There is a group=level of random effect among restaurants
ggplot(sample_store_bars, aes(x=factor(stars), y = average_stars, fill=stars)) +
  geom_boxplot() +
  facet_wrap(~store_name)

```

### 2.3 Linear Model
```{r}
# first fit a simple linear model
fit1 <- lm(stars~review_count_c + useful_pct + average_stars + store_stars + store_review_count_c + store_name, data = sample_store_bars)
display(fit1)
# the linear model is not applicable to factor response
```

### 2.4 Multinomial logistic regression

```{r}
# then try a logistic model
fit.multi <- vglm(ordered(stars) ~ review_count_c + useful_pct + average_stars + store_stars + store_review_count_c, data = sample_store_bars, family=cumulative)
binnedplot(fitted(fit.multi,type="response"), resid(fit.multi, type="response"))
summary(fit.multi)
# so far the residuals look good but I still try the other models to see whether we can find a better one
```


### 2.5 Multilevel Model
```{r}
# not including store_review_count because we sample the data by store review count
fit2 <- glmer(stars ~ review_count_c + useful_pct + average_stars + store_stars + store_review_count_c + (1|store_name), data = sample_store_bars, family=poisson)
summary(fit2)
print(fit2, corr = FALSE)

# binned residual plot
binnedplot(fitted(fit2), resid(fit2))
# ggplot residual plot
ggplot(fit2, aes(x=fitted(fit2), y=resid(fit2))) +
  geom_point()
# most of the residuals are random distributed, it looks the multilevel model is better than the logistic model
se2 <- sqrt(diag(vcov(fit2)))
# table of estimates with 95% CI
(tab2 <- cbind(Est = fixef(fit2), LL = fixef(fit2) - 1.96 * se2, UL = fixef(fit2) + 1.96 * se2))
# useful_pct and Store_review_count have prediction intercal aross zero
```
```{r}
# another way of doing multilevel model
fit3 <- glmer(stars ~ review_count_c + useful_pct + average_stars + store_stars + store_review_count_c + (1|store_name), data = sample_store_bars, family = poisson,control=glmerControl(optimizer="bobyqa"))
summary(fit3)
binnedplot(fitted(fit3), resid(fit3))
ggplot(fit3, aes(fitted(fit3), resid(fit3))) +
  geom_point()
# still not showing a good fit
```

## 3. User as Random Effect
### 3.1 Variable Selection
```{r}
sample_user_bars <- read.csv("sample_user_bars.csv")
# check correlations within predictor variables
ggpairs(sample_user_bars[, c("review_count","useful_pct","fans","average_stars", "store_review_count")])
# we may want to drop fans as it is highly correlated with review_count, as well as useful_pct
```
### 3.2 EDA
```{r}
# most of the user give scores of 4 or 5 to bars
ggplot(sample_user_bars, aes(x = factor(makeup_id), y = stars)) +
  stat_sum(aes(size = ..n.., group = 1), color = "tomato3") +
  theme(axis.text.x=element_text(angle=90, hjust=1, size = 6))
# There is a individual-level of random effect among users
ggplot(sample_user_bars, aes(x=stars, y =store_review_count_c, fill = stars)) +
  geom_boxplot() +
  facet_wrap(~factor(makeup_id))
# 1 means the store is open, 0 means the store closed.
ggplot(sample_user_bars, aes(is_open, stars)) +
  geom_jitter()
# we would include is_open as a level predictor as well.
```
### 3.3 Multilevel Model
```{r}
sample_user_bars$makeup_id <- factor(sample_user_bars$makeup_id)
fit <- glmer(factor(stars) ~ review_count_c + average_stars + store_review_count_c + (1|makeup_id) + (1|is_open), data = sample_user_bars, family = binomial,control=glmerControl(optimizer="bobyqa"))
summary(fit)
print(fit, corr = FALSE)
# our model is improved by including individual random effect
binnedplot(fitted(fit), resid(fit))

se2 <- sqrt(diag(vcov(fit2)))
# table of estimates with 95% CI
(tab <- cbind(Est = fixef(fit2), LL = fixef(fit2) - 1.96 * se2, UL = fixef(fit2) + 1.96 * se2))
```

```{r}
# fitted value v.s. residuals
fit.x <- fit@frame$`factor(stars)`
fit.y <- predict(fit)
fit.X <- data.frame(fit.x, fit.y, "resid" = resid(fit))
# predict v.s observed
# the model give more accurate results when the observed score is around 4
ggplot(fit.X, aes(fit.x, fit.y)) +
    geom_point(position = position_jitter(width = .4)) +
    geom_smooth(method = "loess", 
                se = FALSE) + 
    theme_minimal() + 
    labs(x = "stars", y = "predict")
# residual plots shows the negative residuals are really high
ggplot(fit.X, aes(fit.y, resid)) +
  geom_point(position=position_jitter(width=.4))
```


## Discussion
Althgough from the summary of our model that the coefficients are significant, the prediction plots show otherwise. I think this is due to the limitation of data size. My laptop is not able to process such a large size of levels. There are millions of users and business in the dataset and we build the model by sampling. 


