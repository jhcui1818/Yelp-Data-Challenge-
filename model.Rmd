---
title: "restaurant"
author: "Jinghan Cui"
date: "12/1/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(lme4)
library(rstanarm)
library(arm)
```

## 1. Data Cleaning
```{r, echo=FALSE}
# Using the csv file created by data manipulation
bars <- read.csv("bars.csv")

# remove outliers by plotting the overall data
ggplot(bars, aes(x = store_stars, y = store_review_count))+
  geom_jitter(color = "light blue")
ggplot(bars, aes(x = stars, y = review_count))+
  geom_jitter(color = "light blue")
ggplot(bars, aes(x = stars, y = review_count))+
  geom_jitter(color = "light blue")
bars_reduced <- bars %>%
  filter(review_count >50,review_count<9000, store_review_count>200, store_review_count<3500)

# remove redundant information and reorder the columns to improve effiiency
bars.v2 <- bars_reduced[, c(-1,-3,-4)]
bars.v3 <- bars.v2[,c(1,8,3,4,5,6,7,9,11,12,13,10)]
bars.v3 <- bars.v3[order(bars.v3$makeup_id),]

bars.v3$is_open <- as.factor(bars.v3$is_open)
bars.v3$stars <- as.factor(bars.v3$stars)

bars.v3$useful_pct <- bars.v3$useful/bars.v3$review_count
bars.v3$review_count_c <- (bars.v3$review_count-mean(bars.v3$review_count))/sd(bars.v3$review_count)
bars.v3$fans_c <- (bars.v3$fans-mean(bars.v3$fans))/sd(bars.v3$fans)
bars.v3$store_review_count_c <- (bars.v3$store_review_count-mean(bars.v3$store_review_count))/sd(bars.v3$store_review_count)
bars.v3$useful_pct_c <- (bars.v3$useful_pct-mean(bars.v3$useful_pct))/sd(bars.v3$useful_pct)

```

### 2. Model1
## 2.1 Variable Selection
```{r}
sample_store_bars <- bars.v3 %>%
  filter(store_review_count <2500, store_review_count >2000)
# check correlations within predictor variables
ggpairs(sample_store_bars[, c("review_count","useful_pct","fans","average_stars", "store_review_count")])
# we may want to drop fans as it is highly correlated with review_count
```

## 2.2 EDA
```{r}
# the sampled restaurants have same store_stars while the plot shows a different distribution of reveiw stars per restaurant
ggplot(sample_store_bars, aes(x = store_name, y = stars)) +
  stat_sum(aes(size = ..n.., group = 1), color = "tomato3") 

# There is a group=level of random effect among restaurants
ggplot(sample_store_bars, aes(x=stars, y = average_stars, fill = stars)) +
  geom_boxplot() +
  facet_wrap(~store_name, scales="free_y")
```



## 2.3 Multilevel Model
```{r}
# not including store_review_count because we sample the data by store review count
fit1 <- glmer(factor(stars) ~ review_count_c + useful_pct + average_stars + (1|makeup_id), data = sample_store_bars, family = binomial,control=glmerControl(optimizer="bobyqa"))
summary(fit1)
print(fit1, corr = FALSE)

se1 <- sqrt(diag(vcov(fit1)))
# table of estimates with 95% CI
(tab <- cbind(Est = fixef(fit1), LL = fixef(fit1) - 1.96 * se1, UL = fixef(fit1) + 1.96 * se1))
```


### 3. Model2
## 3.1 Variable Selection
```{r}
# our data set has too many different users thus we sample the users with a makeup_id smaller than 1000.
sample_user_bars <- bars.v3 %>%
  filter(makeup_id <1000)
# check correlations within predictor variables
ggpairs(sample_user_bars[, c("review_count","useful_pct","fans","average_stars", "store_review_count")])
# we may want to drop fans as it is highly correlated with review_count, as well as useful_pct
```
## 3.2 EDA
```{r}
# most of the user give scores of 4 or 5 to bars
ggplot(sample_user_bars, aes(x = factor(makeup_id), y = stars)) +
  stat_sum(aes(size = ..n.., group = 1), color = "tomato3")
# There is a individual-level of random effect among users
ggplot(sample_user_bars, aes(x=stars, y =store_review_count_c, fill = stars)) +
  geom_boxplot() +
  facet_wrap(~factor(makeup_id))
```

## 3.3 Multilevel Model

```{r}
sample_user_bars$makeup_id <- factor(sample_user_bars$makeup_id)
fit2 <- glmer(factor(stars) ~ review_count_c + average_stars + store_review_count_c + (1|store_name) + (1|makeup_id), data = sample_user_bars, family = binomial,control=glmerControl(optimizer="bobyqa"))
summary(fit2)
print(fit2, corr = FALSE)
# oue model is improved by including individual random effect


se2 <- sqrt(diag(vcov(fit2)))
# table of estimates with 95% CI
(tab <- cbind(Est = fixef(fit2), LL = fixef(fit2) - 1.96 * se2, UL = fixef(fit2) + 1.96 * se2))
```
```{r}
fit.x <- fit2@frame$`factor(stars)`
fit.y <- predict(fit2)
fit.X <- data.frame(fit.x, fit.y, "resid" = resid(fit2))
ggplot(fit.X, aes(fit.x, fit.y)) +
    geom_point(position = position_jitter(width = .4)) +
    geom_smooth(method = "loess", 
                se = FALSE) + 
    theme_minimal() + 
    labs(x = "stars", y = "freqhency")
```

### Discussion
Althgough from the summary of our model that the coefficients are significant, the prediction plots show otherwise. I think this is due to the limitation of data size. My laptop is not able to process such a large size of levels. There are millions of users and business in the dataset and we build the model by sampling. 



